{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар по обучению полносвязных нейросетей в Pytorch\n",
    "\n",
    "На этом семинаре мы будем обучать полносвзяную нейроную сеть на данных MNIST - изображениях рукописных цифр.\n",
    "\n",
    "Мы будем использовать библиотеку Pytorch - это одна из двух наиболее популярных и часто используемых библиотек для обучения нейронных сетей.\n",
    "\n",
    "Отметкой __Задание__ помечены ячейки, в которых Вам нужно написать код. Обязательно выполните все остальные ячейки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с данными MNIST: это набор из 60000 изображений цифр размером 28x28. \n",
    "\n",
    "В Pytorch есть своя обертка, позволяющая скачивать MNIST автоматически, но нам будет удобнее скачать его самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные уже разделены на обучающую и тестовую части!\n",
    "\n",
    "Посмотрим на форму всех загруженных переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем несколько объектов обучающей выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3.2))\n",
    "for im in range(10):\n",
    "    plt.subplot(2, 5, im+1)\n",
    "    plt.imshow(X_train[im, 0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так выглядят данные в виде таблицы объекты-признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "\n",
    "При обучении нейронной сети нам понадобится генерировать мини-батчи объектов. Для этого мы воспользуемся специальными инструментами Pytorch. Мы сократим набор данных для обучения, чтобы оно шло быстрее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(X, y, batch_size=64):\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X).\\\n",
    "                                                reshape(-1, 28*28).float(), \n",
    "                                           torch.from_numpy(y).long())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                               batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = get_loader(X_train[:1500], y_train[:1500])\n",
    "test_loader = get_loader(X_train[1500:3000], y_train[1500:3000])\n",
    "\n",
    "#train_loader = get_loader(X_train, y_train) \n",
    "#test_loader = get_loader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка нейронной сети\n",
    "\n",
    "Нейронные сети обычно \"собирают\" из готовых слоев. В методе \\_\\_init\\_\\_ нужно указать все слои, которые будут использованы, а в методе forward нужно указать, как выполняется _проход вперед_. Иными словами, какие преобразования нужно выполнить, чтобы получить предсказания.\n",
    "\n",
    "__Задание 1.__\n",
    "Составьте нейронную сеть со следующей архитектурой: \n",
    "* Входной слой размера 28*28 нейронов (создавать как отдельную переменнуэ не нужно)\n",
    "* Полносвязный слой с 30 нейронами\n",
    "* Нелинейность ReLU\n",
    "* Полносязный слой с 10 нейронами (по числу классов)\n",
    "\n",
    "\n",
    "В методе \\_\\_init\\_\\_ создайте переменные для хранения всех используемых слоев. \n",
    "\n",
    "Вам понадобятся некоторые слои из следующего набора: nn.Linear, nn.Conv2d, nn.ReLU, nn.Sigmoid.\n",
    "\n",
    "Пример создания переменной для хранения слоя:\n",
    "\n",
    "self.fc1 = nn.Linear(90, 12), где 90 - число входных нейронов и 12 - число выходных нейронов.\n",
    "\n",
    "В методе forward укажите, в какой последовательности нужно применять слои к входной переменной x. \n",
    "\n",
    "Пример применения созданного выше слоя: y = self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurFirstNet(nn.Module):\n",
    "    def __init__(self, k=1):\n",
    "        super(OurFirstNet, self).__init__()\n",
    "        # Ваш код здесь\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ваш код здесь\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем параметры нейросети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = OurFirstNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(param.data.numpy().size for param \\\n",
    "               in model.parameters() if param.requires_grad)\n",
    "\n",
    "count_parameters(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функционал качества (он включает слой Softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__\n",
    "\n",
    "Задайте оптимизатор, реализующий стохастический градиентный спуск, для обучения Вашей нейронной сети. Используйте torch.optim.SGD, в качестве первого его аргумента используйте our_model.parameters(), в качестве второго задайте длину шага, равную 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во время обучения модели удобно контролировать качество и на обучении, и на тесте - возникает дублирующий код. Поэтому мы вынесем в отдельную функцию оценку модели, и в отдельную функцию - одну эпоху обучения (один проход по всей обучающей выборке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, criterion):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch) # проход вперед (выполнение предсказаий)\n",
    "        \n",
    "        loss = criterion(output, y_batch) # вычисление функционала качества\n",
    "        loss.backward() # проход назад (вычисление градиентнов)\n",
    "        optimizer.step() # шаг стохастического градиентного спуска (обновление параметров)\n",
    "\n",
    "def evaluate_loss_acc(model, loader, criterion):\n",
    "    with torch.no_grad(): # сообщаем библиотеке, что вычисление градиентов не потребуется\n",
    "        cumloss, cumacc = 0, 0\n",
    "        num_objects = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            output = model(x_batch) # проход вперед (выполнение предсказаий)\n",
    "            loss = criterion(output, y_batch) # вычисление функционала качества\n",
    "            pred = torch.max(output, 1)[1] # выбор предсказанных классов\n",
    "            acc = torch.sum(pred == y_batch) # вычисление точности \n",
    "\n",
    "            cumloss += loss.item()\n",
    "            cumacc += acc.item()\n",
    "            num_objects += len(x_batch)\n",
    "    return cumloss / num_objects, cumacc / num_objects\n",
    "    \n",
    "    \n",
    "def train(model, opt, train_loader, test_loader, criterion, \\\n",
    "          n_epochs=10, print_info=True):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_epoch(model, opt, train_loader, criterion)\n",
    "        train_loss, train_acc = evaluate_loss_acc(model, train_loader, \n",
    "                                                  criterion)\n",
    "        val_loss, val_acc = evaluate_loss_acc(model, test_loader, \n",
    "                                              criterion)\n",
    "\n",
    "        train_log.append(train_loss)\n",
    "        train_acc_log.append(train_acc)\n",
    "\n",
    "        val_log.append(val_loss)\n",
    "        val_acc_log.append(val_acc)\n",
    "        \n",
    "        if print_info:\n",
    "            print (('Эпоха [%d/%d], функц. кач. (train/test): %.4f/%.4f,'+\\\n",
    "               ' точность (train/test): %.4f/%.4f' )\n",
    "                   %(epoch+1, n_epochs, \\\n",
    "                     train_loss, val_loss, train_acc, val_acc))\n",
    "            \n",
    "    return train_log, train_acc_log, val_log, val_acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 3.__\n",
    "\n",
    "Обучите Вашу нейронную сеть. Для этого воспользуйтесь функцией train, передав в нее следующие аргументы (в указанном порядке):\n",
    "* модель (см. задание 1)\n",
    "* оптимизатор (см. задание 2)\n",
    "* train_loader, test_loader, criterion,\n",
    "* число эпох (используйте 20, можете попробовать значения до 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные задания:\n",
    "* Попробуйте использовать больше нейронов в каждом слое; больше слоев. Найдите конфигурацию, достигающего наивысшего тестового качетва (accuracy).\n",
    "* Для нейросети большего размера (например, 3 слоя по 100 нейронов) попробуйте использовать большее число эпох. Большие нейросети без регуляризации обычно начинают переобучаться, начиная с некоторой эпохи, то есть их качества начинает падать."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
